{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../configs/user_config.yaml\") as f:\n",
    "    model_config = yaml.safe_load(f)\n",
    "\n",
    "load_dotenv(\"../configs/environment_variables.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge base Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defaults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"ch3_data\"\n",
    "DATA_PATH = \"../../data/ch3_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#import azure.ai.resources.client\n",
    "from azure.ai.generative.index import build_index\n",
    "from azure.ai.resources.client import AIClient\n",
    "from azure.ai.resources.operations._index_data_source import (\n",
    "    ACSOutputConfig,\n",
    "    LocalSource,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.aio import SearchClient\n",
    "from azure.search.documents.models import RawVectorQuery\n",
    "from openai import AsyncAzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `build_cogsearch_index` creates the index `INDEX_NAME` using the data present in `DATA_PATH`. \n",
    "\n",
    "- The entire text extracted from the PDF will be converted to smaller managable segments of text referred to as chunks. \n",
    "\n",
    "- `chunk_size` is the number of tokens present in each chunk. And, `chunk_overlap` is the number of overlapping tokens between adjacent chuncks.\n",
    "\n",
    "- After splitting the text into smaller chuncks, these chuncks will be converted to numberical representation of the text using an Azure Openai embedding model. This are referred to as vector embeddings. \n",
    "\n",
    "- Embeddings are stored in the `vector_store`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cogsearch_index(\n",
    "    index_name: str,\n",
    "    path_to_data: str,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    data_source_url: str = None,\n",
    "):\n",
    "    # Set up environment variables for cog search SDK\n",
    "    os.environ[\"AZURE_COGNITIVE_SEARCH_TARGET\"] = os.environ.get(\n",
    "        \"AZURE_AI_SEARCH_ENDPOINT\", \"\"\n",
    "    )\n",
    "    os.environ[\"AZURE_COGNITIVE_SEARCH_KEY\"] = os.environ.get(\"AZURE_AI_SEARCH_KEY\", \"\")\n",
    "\n",
    "    client = AIClient.from_config(DefaultAzureCredential())\n",
    "\n",
    "    #default_aoai_connection = client.get_default_aoai_connection()\n",
    "    default_aoai_connection = client._connections.get(os.environ.get(\"AZURE_OPENAI_CONNECTION\", \"\"))\n",
    "    default_aoai_connection.set_current_environment()\n",
    "\n",
    "    default_acs_connection = client.connections.get(\n",
    "        os.environ.get(\"AZURE_COGNITIVE_SEARCH_CONNECTION_NAME\", \"\")\n",
    "    )\n",
    "    default_acs_connection.set_current_environment()\n",
    "\n",
    "    # Use the same index name when registering the index in AI Studio\n",
    "    index = build_index(\n",
    "        output_index_name=index_name,\n",
    "        vector_store=\"azure_cognitive_search\",\n",
    "        embeddings_model=f\"azure_open_ai://deployment/{os.environ.get('AZURE_OPENAI_EMBEDDING_DEPLOYMENT')}/model/{os.environ.get('AZURE_OPENAI_EMBEDDING_MODEL')}\",\n",
    "        data_source_url=data_source_url,\n",
    "        index_input_config=LocalSource(input_data=path_to_data),\n",
    "        acs_config=ACSOutputConfig(\n",
    "            acs_index_name=index_name,\n",
    "        ),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "    )\n",
    "\n",
    "    # register the index so that it shows up in the project\n",
    "    cloud_index = client.indexes.create_or_update(index)\n",
    "\n",
    "    # update user-config to add index name\n",
    "    with open(\"../configs/user_config.yaml\", 'w') as f:\n",
    "        model_config[\"rag\"][\"index_name\"] = cloud_index.name\n",
    "        yaml.safe_dump(model_config, f)\n",
    "\n",
    "    print(f\"Created index '{cloud_index.name}'\")\n",
    "    print(f\"Local Path: {index.path}\")\n",
    "    print(f\"Cloud Path: {cloud_index.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AIClient: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "[DocumentChunksIterator::filter_extensions] Filtered 0 files out of 1\n",
      "[DocumentChunksIterator::crack_documents] Total time to load files: 6.651878356933594e-05\n",
      "{\n",
      "  \".txt\": 0.0,\n",
      "  \".md\": 0.0,\n",
      "  \".html\": 0.0,\n",
      "  \".htm\": 0.0,\n",
      "  \".py\": 0.0,\n",
      "  \".pdf\": 1.0,\n",
      "  \".ppt\": 0.0,\n",
      "  \".pptx\": 0.0,\n",
      "  \".doc\": 0.0,\n",
      "  \".docx\": 0.0,\n",
      "  \".xls\": 0.0,\n",
      "  \".xlsx\": 0.0\n",
      "}\n",
      "[DocumentChunksIterator::split_documents] Total time to split 62 documents into 161 chunks: 0.9729752540588379\n",
      "Processing document: CH3-data.pdf0\n",
      "Processing document: CH3-data.pdf1\n",
      "Processing document: CH3-data.pdf2\n",
      "Processing document: CH3-data.pdf3\n",
      "Processing document: CH3-data.pdf4\n",
      "Processing document: CH3-data.pdf5\n",
      "Processing document: CH3-data.pdf6\n",
      "Processing document: CH3-data.pdf7\n",
      "Processing document: CH3-data.pdf8\n",
      "Processing document: CH3-data.pdf9\n",
      "Processing document: CH3-data.pdf10\n",
      "Processing document: CH3-data.pdf11\n",
      "Processing document: CH3-data.pdf12\n",
      "Processing document: CH3-data.pdf13\n",
      "Processing document: CH3-data.pdf14\n",
      "Processing document: CH3-data.pdf15\n",
      "Processing document: CH3-data.pdf16\n",
      "Processing document: CH3-data.pdf17\n",
      "Processing document: CH3-data.pdf18\n",
      "Processing document: CH3-data.pdf19\n",
      "Processing document: CH3-data.pdf20\n",
      "Processing document: CH3-data.pdf21\n",
      "Processing document: CH3-data.pdf22\n",
      "Processing document: CH3-data.pdf23\n",
      "Processing document: CH3-data.pdf24\n",
      "Processing document: CH3-data.pdf25\n",
      "Processing document: CH3-data.pdf26\n",
      "Processing document: CH3-data.pdf27\n",
      "Processing document: CH3-data.pdf28\n",
      "Processing document: CH3-data.pdf29\n",
      "Processing document: CH3-data.pdf30\n",
      "Processing document: CH3-data.pdf31\n",
      "Processing document: CH3-data.pdf32\n",
      "Processing document: CH3-data.pdf33\n",
      "Processing document: CH3-data.pdf34\n",
      "Processing document: CH3-data.pdf35\n",
      "Processing document: CH3-data.pdf36\n",
      "Processing document: CH3-data.pdf37\n",
      "Processing document: CH3-data.pdf38\n",
      "Processing document: CH3-data.pdf39\n",
      "Processing document: CH3-data.pdf40\n",
      "Processing document: CH3-data.pdf41\n",
      "Processing document: CH3-data.pdf42\n",
      "Processing document: CH3-data.pdf43\n",
      "Processing document: CH3-data.pdf44\n",
      "Processing document: CH3-data.pdf45\n",
      "Processing document: CH3-data.pdf46\n",
      "Processing document: CH3-data.pdf47\n",
      "Processing document: CH3-data.pdf48\n",
      "Processing document: CH3-data.pdf49\n",
      "Processing document: CH3-data.pdf50\n",
      "Processing document: CH3-data.pdf51\n",
      "Processing document: CH3-data.pdf52\n",
      "Processing document: CH3-data.pdf53\n",
      "Processing document: CH3-data.pdf54\n",
      "Processing document: CH3-data.pdf55\n",
      "Processing document: CH3-data.pdf56\n",
      "Processing document: CH3-data.pdf57\n",
      "Processing document: CH3-data.pdf58\n",
      "Processing document: CH3-data.pdf59\n",
      "Processing document: CH3-data.pdf60\n",
      "Processing document: CH3-data.pdf61\n",
      "Processing document: CH3-data.pdf62\n",
      "Processing document: CH3-data.pdf63\n",
      "Processing document: CH3-data.pdf64\n",
      "Processing document: CH3-data.pdf65\n",
      "Processing document: CH3-data.pdf66\n",
      "Processing document: CH3-data.pdf67\n",
      "Processing document: CH3-data.pdf68\n",
      "Processing document: CH3-data.pdf69\n",
      "Processing document: CH3-data.pdf70\n",
      "Processing document: CH3-data.pdf71\n",
      "Processing document: CH3-data.pdf72\n",
      "Processing document: CH3-data.pdf73\n",
      "Processing document: CH3-data.pdf74\n",
      "Processing document: CH3-data.pdf75\n",
      "Processing document: CH3-data.pdf76\n",
      "Processing document: CH3-data.pdf77\n",
      "Processing document: CH3-data.pdf78\n",
      "Processing document: CH3-data.pdf79\n",
      "Processing document: CH3-data.pdf80\n",
      "Processing document: CH3-data.pdf81\n",
      "Processing document: CH3-data.pdf82\n",
      "Processing document: CH3-data.pdf83\n",
      "Processing document: CH3-data.pdf84\n",
      "Processing document: CH3-data.pdf85\n",
      "Processing document: CH3-data.pdf86\n",
      "Processing document: CH3-data.pdf87\n",
      "Processing document: CH3-data.pdf88\n",
      "Processing document: CH3-data.pdf89\n",
      "Processing document: CH3-data.pdf90\n",
      "Processing document: CH3-data.pdf91\n",
      "Processing document: CH3-data.pdf92\n",
      "Processing document: CH3-data.pdf93\n",
      "Processing document: CH3-data.pdf94\n",
      "Processing document: CH3-data.pdf95\n",
      "Processing document: CH3-data.pdf96\n",
      "Processing document: CH3-data.pdf97\n",
      "Processing document: CH3-data.pdf98\n",
      "Processing document: CH3-data.pdf99\n",
      "Processing document: CH3-data.pdf100\n",
      "Processing document: CH3-data.pdf101\n",
      "Processing document: CH3-data.pdf102\n",
      "Processing document: CH3-data.pdf103\n",
      "Processing document: CH3-data.pdf104\n",
      "Processing document: CH3-data.pdf105\n",
      "Processing document: CH3-data.pdf106\n",
      "Processing document: CH3-data.pdf107\n",
      "Processing document: CH3-data.pdf108\n",
      "Processing document: CH3-data.pdf109\n",
      "Processing document: CH3-data.pdf110\n",
      "Processing document: CH3-data.pdf111\n",
      "Processing document: CH3-data.pdf112\n",
      "Processing document: CH3-data.pdf113\n",
      "Processing document: CH3-data.pdf114\n",
      "Processing document: CH3-data.pdf115\n",
      "Processing document: CH3-data.pdf116\n",
      "Processing document: CH3-data.pdf117\n",
      "Processing document: CH3-data.pdf118\n",
      "Processing document: CH3-data.pdf119\n",
      "Processing document: CH3-data.pdf120\n",
      "Processing document: CH3-data.pdf121\n",
      "Processing document: CH3-data.pdf122\n",
      "Processing document: CH3-data.pdf123\n",
      "Processing document: CH3-data.pdf124\n",
      "Processing document: CH3-data.pdf125\n",
      "Processing document: CH3-data.pdf126\n",
      "Processing document: CH3-data.pdf127\n",
      "Processing document: CH3-data.pdf128\n",
      "Processing document: CH3-data.pdf129\n",
      "Processing document: CH3-data.pdf130\n",
      "Processing document: CH3-data.pdf131\n",
      "Processing document: CH3-data.pdf132\n",
      "Processing document: CH3-data.pdf133\n",
      "Processing document: CH3-data.pdf134\n",
      "Processing document: CH3-data.pdf135\n",
      "Processing document: CH3-data.pdf136\n",
      "Processing document: CH3-data.pdf137\n",
      "Processing document: CH3-data.pdf138\n",
      "Processing document: CH3-data.pdf139\n",
      "Processing document: CH3-data.pdf140\n",
      "Processing document: CH3-data.pdf141\n",
      "Processing document: CH3-data.pdf142\n",
      "Processing document: CH3-data.pdf143\n",
      "Processing document: CH3-data.pdf144\n",
      "Processing document: CH3-data.pdf145\n",
      "Processing document: CH3-data.pdf146\n",
      "Processing document: CH3-data.pdf147\n",
      "Processing document: CH3-data.pdf148\n",
      "Processing document: CH3-data.pdf149\n",
      "Processing document: CH3-data.pdf150\n",
      "Processing document: CH3-data.pdf151\n",
      "Processing document: CH3-data.pdf152\n",
      "Processing document: CH3-data.pdf153\n",
      "Processing document: CH3-data.pdf154\n",
      "Processing document: CH3-data.pdf155\n",
      "Processing document: CH3-data.pdf156\n",
      "Processing document: CH3-data.pdf157\n",
      "Processing document: CH3-data.pdf158\n",
      "Processing document: CH3-data.pdf159\n",
      "Processing document: CH3-data.pdf160\n",
      "Documents to embed: 161\n",
      "Documents reused: 0\n",
      "Attempt 0 to embed 16 documents.\n",
      "Attempt 0 to embed 16 documents.\n",
      "Attempt 0 to embed 16 documents.\n",
      "Attempt 0 to embed 16 documents.\n",
      "Attempt 0 to embed 16 documents.\n",
      "Attempt 0 to embed 16 documents.\n",
      "Attempt 0 to embed 16 documents.\n",
      "Attempt 0 to embed 16 documents.\n",
      "Attempt 0 to embed 16 documents.\n",
      "Attempt 0 to embed 16 documents.\n",
      "Attempt 0 to embed 1 documents.\n",
      "Updating ACS index\n",
      "Using Index fields: {\n",
      "  \"content\": \"content\",\n",
      "  \"url\": \"url\",\n",
      "  \"filename\": \"filepath\",\n",
      "  \"title\": \"title\",\n",
      "  \"metadata\": \"meta_json_string\",\n",
      "  \"embedding\": \"contentVector\"\n",
      "}\n",
      "Ensuring search index ch3_data exists\n",
      "Search index ch3_data already exists\n",
      "0 documents from sources marked for deletion, adding individual documents marked for deletion\n",
      "Total 0 documents marked for deletion\n",
      "Documents include embeddings: True\n",
      "Processing documents from: CH3-data\n",
      "Sending 100 documents to ACS\n",
      "Uploaded 100 documents to ACS in 2.4182 seconds, 0 failed\n",
      "Uploaded documents\n",
      "Sending 61 documents to ACS\n",
      "Uploaded 61 documents to ACS in 0.6913 seconds, 0 failed\n",
      "Uploaded documents\n",
      "Built index from 161 documents and 161 chunks, took 3.1276 seconds\n",
      "Built index\n",
      "Writing MLIndex yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index 'ch3_data'\n",
      "Local Path: /home/shailesh_sharma/tiger_hackathon/hacks-main/codes/notebooks/ch3_data-mlindex\n",
      "Cloud Path: azureml://subscriptions/57a36344-3906-4293-9991-5010c5255d5e/resourcegroups/rg-shailesh.sharma_ai/workspaces/ai-build-shaileshsharma-v1/datastores/workspaceblobstore/paths/LocalUpload/a221550659575a0681c6758820ede2f6/ch3_data-mlindex/\n"
     ]
    }
   ],
   "source": [
    "build_cogsearch_index(\n",
    "    index_name=INDEX_NAME,\n",
    "    path_to_data=DATA_PATH,\n",
    "    chunk_size=model_config[\"rag\"][\"chunk_size\"],\n",
    "    chunk_overlap=model_config[\"rag\"][\"chunk_overlap\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with Documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import List\n",
    "\n",
    "import nest_asyncio\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `get_documents` function takes in user question and identifies the top 5 (`num_docs`) chuncks that are most relevant to the user question. \n",
    "\n",
    "- The `question` is converted to vector embeddings using the same embedding model used while creating the index. This vector is compared with the embeddings stored in the vector store, to retieve the top few chunks based on similarity scores. \n",
    "\n",
    "- These retrieved chuncks will be refered to as context to the Azure Openai LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_documents(\n",
    "    question: str,\n",
    "    index_name: str,\n",
    "    num_docs=5,\n",
    ") -> str:\n",
    "    #  retrieve documents relevant to the user's question from Cognitive Search\n",
    "    search_client = SearchClient(\n",
    "        endpoint=os.environ.get(\"AZURE_AI_SEARCH_ENDPOINT\", \"\"),\n",
    "        credential=AzureKeyCredential(os.environ.get(\"AZURE_AI_SEARCH_KEY\", \"\")),\n",
    "        index_name=index_name,\n",
    "    )\n",
    "\n",
    "    async with AsyncAzureOpenAI(\n",
    "        azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_KEY\", \"\"),\n",
    "        api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"\"),\n",
    "    ) as aclient:\n",
    "\n",
    "        # generate a vector embedding of the user's question\n",
    "        embedding = await aclient.embeddings.create(\n",
    "            input=question, model=os.environ.get(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "        )\n",
    "        embedding_to_query = embedding.data[0].embedding\n",
    "\n",
    "    context = \"\"\n",
    "    contexts = []\n",
    "    async with search_client:\n",
    "        # use the vector embedding to do a vector search on the index\n",
    "        vector_query = RawVectorQuery(\n",
    "            vector=embedding_to_query, k=num_docs, fields=\"contentVector\"\n",
    "        )\n",
    "        results = await search_client.search(\n",
    "            search_text=\"\", vector_queries=[vector_query], select=[\"id\", \"content\"]\n",
    "        )\n",
    "\n",
    "        async for result in results:\n",
    "            context += f\"\\n>>> {result['content']}\"\n",
    "            contexts.append(result[\"content\"])\n",
    "\n",
    "    return context, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Following is the prompt template for Azure Openai GPT 3.5 turbo model\n",
    "    ```python\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_role},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    ```\n",
    "\n",
    "- The `system_role` and `user_prompt` is defined in the user config. This would be the input to the LLM, and it whould contain placeholders for `question`, and the `context` retrieved in the previous step.\n",
    "\n",
    "- Upon passing this to the LLM, we get a response in the question based on the context provided from the input documents. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_message(user_prompt: str, system_role: str) -> List[dict]:\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "\n",
    "def chat_completion(\n",
    "    question: str,\n",
    "    system_role: str,\n",
    "    user_prompt: str,\n",
    "    index_name: str,\n",
    "    num_docs: int = 5,\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int = 800,\n",
    "):\n",
    "    # get search documents for the last user message in the conversation\n",
    "    context, contexts = asyncio.run(\n",
    "        get_documents(\n",
    "            question=question,\n",
    "            index_name=index_name,\n",
    "            num_docs=num_docs,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # TODO: Add context to user message\n",
    "    user_prompt = user_prompt.format(question=question, context=context)\n",
    "    message = build_message(user_prompt=user_prompt, system_role=system_role)\n",
    "\n",
    "    with AzureOpenAI(\n",
    "        azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_KEY\", \"\"),\n",
    "        api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"\"),\n",
    "    ) as client:\n",
    "\n",
    "        # call Azure OpenAI with the system prompt and user's question\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT\"),\n",
    "            messages=message,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "\n",
    "    response = {\n",
    "        \"choices\": [\n",
    "            {\n",
    "                \"index\": 0,\n",
    "                \"message\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": chat_completion.choices[0].message.content,\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # add context in the returned response\n",
    "    context_dict = {\n",
    "        \"context\": context,\n",
    "        \"contexts\": contexts,\n",
    "        \"num_docs\": num_docs,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    response[\"choices\"][0][\"context\"] = context_dict\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_documents(question: str):\n",
    "    result = chat_completion(\n",
    "        question=question,\n",
    "        system_role=model_config[\"prompt\"][\"system_role\"],\n",
    "        user_prompt=model_config[\"prompt\"][\"user_prompt\"]\n",
    "        + \"\\n\\nQuestion:'{question}' \\n\\nContext: '{context}'\",\n",
    "        index_name=INDEX_NAME,\n",
    "        num_docs=model_config[\"rag\"][\"num_docs\"],\n",
    "        temperature=model_config[\"model\"][\"temperature\"],\n",
    "        max_tokens=model_config[\"model\"][\"max_tokens\"],\n",
    "    )\n",
    "    print(result[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question and Answering on the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, Formula 1 introduced limitations on the number of upgrades teams could make to their power units during the season to achieve the long-term competitive balance, sporting fairness, and financial stability of the Championship in respect of power units. These limitations aim to preserve the unique technology and engineering challenge of Formula 1 while also ensuring that power unit manufacturers allocate their resources efficiently within the power unit cost cap.\n"
     ]
    }
   ],
   "source": [
    "chat_with_documents(\n",
    "    question=\"Why did Formula 1 introduce limitations on the number of upgrades teams could make to their power units during the season?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureaistudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
